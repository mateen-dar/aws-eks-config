import sys
import json
import boto3
import os
import logging
from io import StringIO
from datetime import datetime
import traceback
import concurrent.futures
import urllib.parse
sys.path.append(os.path.join(os.path.dirname(__file__), 'package'))
from urllib.parse import urlparse

import asyncio
import httpx
from markdown import Markdown
import requests
from bs4 import BeautifulSoup
import imghdr
import concurrent.futures
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def parse_images_search_results(content):
    soup = BeautifulSoup(content, "html.parser")
    articles = soup.find_all("article", class_="result result-images category-images")
    urls = [
        a["href"]
        for article in articles
        for a in article.find_all("a", href=True)
        if a["href"].lower().endswith((".jpg", ".jpeg"))
    ]
    unique_urls = list(set(urls))

    print(unique_urls)
    return unique_urls[:20]




#parses search results
def parse_search_result(content):
    soup = BeautifulSoup(content, "html.parser")
    # Find all articles under the news category
    articles = soup.select("article.result.result-default.category-general")

    # Track unique domains
    unique_domains = set()
    unique_urls = []

    for article in articles:
        a_tag = article.find("a", class_="url_header")
        if a_tag and a_tag.has_attr("href"):
            url = a_tag["href"]
            domain = urlparse(url).netloc
            if domain not in unique_domains:
                unique_domains.add(domain)
                unique_urls.append(url)

    return unique_urls[:5]

user_prompt = None
# Setup
brt = boto3.client(service_name='bedrock-runtime')

# Logging
logging.basicConfig(
    level=logging.DEBUG if os.environ.get("DEBUG", False) else logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
    datefmt="%d-%b-%y %H:%M",
    force=True
)
log = logging.getLogger("Run-Lambda")
log.info("Execution Started")

#Returns Bytes and the format for a Image URL
def get_image_bytes_and_format(image_url):
    try:
        response = requests.get(image_url, verify=False)
        response.raise_for_status()
        content = response.content
        fmt = imghdr.what(None, content)
        return fmt , content
    except requests.exceptions.RequestException as e:
        return None, None
    

#Proceses each Image trough a LLM to see if the questions is related to this image
def is_this_image_contextually_valid(user_question, image_url):
    model_id = "us.meta.llama3-2-90b-instruct-v1:0"
    image_fmt, image_bytes = get_image_bytes_and_format(image_url)
    if image_bytes and image_fmt:
        user_message = f'''
        This is an image that has been scraped from the web. I want you to analyze the image, and tell me if this image could be helpful in answering the question: "{user_question}".
        I want you to focus on the topic of the question and not the question iteself. If the question is about a product or a celebrity, I want you to see if the image is about that particular celebrity or Product.
        If you feel this is contextually valid, I want you to reply with True else I want you to reply with False.
        *** you should only reply in True or False, nothing else ***
        '''
        messages = [
            {
                "role": "user",
                "content": [
                    {"image": {"format": image_fmt, "source": {"bytes": image_bytes}}},
                    {"text": user_message},
                ],
            }
        ]
        try:
            response = brt.converse(
                modelId=model_id,
                messages=messages,
            )
            return (image_url, response["output"]["message"]["content"][0]["text"])
        except Exception as e:
            return (image_url, "Error calling model")
    else:
        return (image_url, "Invalid image")
    
# Markdown cleaning
def unmark_element(element, stream=None):
    if stream is None:
        stream = StringIO()
    if element.text:
        stream.write(element.text)
    for sub in element:
        unmark_element(sub, stream)
    if element.tail:
        stream.write(element.tail)
    return stream.getvalue()

Markdown.output_formats["plain"] = unmark_element
__md = Markdown(output_format="plain")
__md.stripTopLevelTags = False

def unmark(text: str) -> str:
    return __md.convert(text)

# Claude 3.5 Haiku summarizer
def create_summary_claude_haiku_3_5(scraped_text):
    prompt = f'''
    You are a text cleaning and summarization expert.
    I will give you raw text scraped from a website. This text may contain:
    - advertisements
    - cookie banners
    - UI elements (buttons, menus, footers)
    - terms of service, privacy notices
    - unrelated promotional text
    - duplicated content
    - navigation labels

    Your task:
    1. Remove all irrelevant, redundant, or boilerplate text that is not relevant to the question {user_prompt}
    2. Keep only the **main informative content** of the page — the key points and the useful information a human would want to retain. Keep all content relevant to the question {user_prompt}
    3. **DO NOT** summarize the text in any fashion.
    4. **DO NOT** shorten the text in any fashion.
    5. Extract the key technology trends represented in the headlines
    6. Extract the most prominent stories in as much detail as possible
    7. Extract overview of the tech news categories
    8. Write the cleaned text in clear, human-readable paragraphs that is atleast 100 words long.
    9. DO NOT include section titles like "Terms", "Advertisement", "Sponsor Center", etc.
    10. Do not refer to "this site", "this page", or "the website" in your output — just state the relevant content.
    11. If the scraped text contains financial data, company description, key metrics, market trends, or analyst commentary — preserve that.
    12. Ignore anything that sounds like user interface labels (like "Show More", "Read More", "Login", "Privacy Center", etc).


    Input:
    {scraped_text}

    Output:
    cleaned_summary_of_page
    '''

    model_id = "us.anthropic.claude-3-5-haiku-20241022-v1:0"
    native_request = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 1000,
        "temperature": 0.5,
        "messages": [
            {
                "role": "user",
                "content": [{"type": "text", "text": prompt}]
            }
        ],
    }

    response = brt.invoke_model(
        modelId=model_id,
        body=json.dumps(native_request)
    )
    model_response = json.loads(response["body"].read())
    response_text = model_response["content"][0]["text"]

    return response_text

# Process individual item
def process_item(item):
    summary = create_summary_claude_haiku_3_5(item["results"])
    item["results"] = summary
    return item

# Crawler
async def crawl(client: httpx.AsyncClient, url: str) -> dict:
    resp = await client.post(
        "https://crawl4ai-poc.analytics-dev.tapestry.com/crawl",
        json={
            "urls": [url],
            "crawler_config": {
                "type": "CrawlerRunConfig",
                "params": {
                    "word_count_threshold": 15,
                    "excluded_tags": ["nav", "header", "footer", "script", "style"],
                    "scraping_strategy": {"type": "WebScrapingStrategy", "params": {}},
                    "exclude_social_media_domains": [
                        "facebook.com", "twitter.com", "x.com", "linkedin.com",
                        "instagram.com", "pinterest.com", "tiktok.com",
                        "snapchat.com", "reddit.com"
                    ],
                    "exclude_external_links": True
                }
            }
        }
    )
    resp.raise_for_status()
    return resp.json()

# Async crawl multiple URLs
async def async_crawl(urls):
    timeout = httpx.Timeout(150.0)
    async with httpx.AsyncClient(timeout=timeout) as client:
        tasks = [crawl(client, url) for url in urls]
        all_results = await asyncio.gather(*tasks, return_exceptions=True)

    output = []
    for result in all_results:
        if isinstance(result, Exception):
            continue
        for entry in result.get("results", []):
            try:

                images=[]
                raw_md = entry["markdown"]["raw_markdown"]
                cleaned = raw_md
                output.append({
                    "url": entry.get("url"),
                    "results": cleaned,
                    "images":images
                })
            except Exception:
                log.error("Exception occurred reading URL", exc_info=True)

    return output

# Prompt generator for search engine
def get_se_prompt():
    return '''<SYSTEM>
            You are an expert search query generator for a web scraping agent. Your goal is to break down the user's question into optimized search strings that will help retrieve broad and deep information from DuckDuckGo.
            Important guidelines:
            1. Generate 3 distinct search strings that cover **both direct and indirect aspects** of the user's query.
            2. Remember all searches should be relevant to the **current year 2025**.
            2. Include **related concepts**, **contextual angles**, and **alternative perspectives** that will enrich the results.
                - Example: If the user asks for the stock price of a company, generate queries for "6-month stock analysis", "recent earnings reports", "market sentiment", "competitor comparison", etc.
                - Example: If the user asks about a city, generate queries for "sister cities", "current tourism trends", "economic development", "local industries", etc.
                - Example: If the user asks about a product, generate queries for "alternatives", "recent reviews", "industry trends", "buyer demographics", etc.
            3. Focus on factual, current information retrieval.
            4. Always include **core keywords** from the user's original query in at least 3 of the search strings.
            5. Prioritize search strings that will lead to **authoritative and high-quality sources**.
            6. Format each search string to work effectively with Google, Bing and DuckGoGo.
            7. Do not include any explanations or commentary — output only the search strings.
            8. Ensure the JSON is minified, valid, and directly parsable with Python's json.loads().

            </SYSTEM>

            <TASK>
            Analyze the user's question and generate specific search strings that will retrieve both direct and complementary information from the web via DuckDuckGo.
            </TASK>

            <FORMAT>
            {"search_strings":["string1","string2"]}
            </FORMAT>
'''

# Invoke Claude 3.7 Sonnet
def invoke_model(user_question):
    body = json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 500,
        "system": get_se_prompt(),
        "messages": [{"role": "user", "content": user_question}]
    })

    response = brt.invoke_model(body=body, modelId='us.anthropic.claude-3-7-sonnet-20250219-v1:0')
    response_body = json.loads(response.get('body').read())
    return response_body['content'][0]['text']

# Utility
def get_current_month_year():
    now = datetime.now()
    return 'for ' + now.strftime("%B, %Y")

# Main Lambda handler
def lambda_handler(event, context):
    log.info("Lambda handler invoked")

    agent = event.get('agent')
    action_group = event.get('actionGroup')
    function = event.get('function')
    parameters = event.get('parameters', [])
    user_prompt = parameters[0]['value']
    include_images  = parameters[1]['value']



    print(parameters)

    response_text = invoke_model(user_prompt)

    try:
        search_responses = json.loads(response_text)['search_strings']
        log.info(f"Search strings: {search_responses}")
    except json.JSONDecodeError as e:
        log.error(f"JSON parsing error: {e}")
        return {"error": "Failed to parse JSON from model output."}

    urls = []
    unique_domains = set()

    for query_string in search_responses:
        encoded_query = urllib.parse.quote(f"{query_string} {get_current_month_year()}")
        response = requests.post(
            f"https://sear-eng-poc.analytics-dev.tapestry.com/search?q={encoded_query}&language=auto&time_range=&safesearch=0&categories=general",
            timeout=10,
            verify=False
        )
        results = parse_search_result(response.text)
        for url in results:
            domain = urlparse(url).netloc
            if domain not in unique_domains:
                unique_domains.add(domain)
                urls.append(url)
       
    log.info(f"Retrieved URLs: {urls}")

    # Filter out unwanted URLs
    urls = [url for url in urls if "wikipedia" not in url.lower()]

    # Run async crawl
    output = asyncio.run(async_crawl(urls))

    # Process items concurrently
    updated_items = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(process_item, item) for item in output]
        for future in concurrent.futures.as_completed(futures):
            updated_item = future.result()
            updated_items.append(updated_item)

    # Log and prepare final response
    log.info(f"Processed {len(updated_items)} items")

    summary_content = "\n\n".join([item["results"] for item in updated_items])
    all_urls = [item.get("url") for item in updated_items if item.get("url")]
    all_images_url = []

    encoded_query = urllib.parse.quote(f"{user_prompt} {get_current_month_year()}")
    response = requests.post(
        f"https://sear-eng-poc.analytics-dev.tapestry.com/search?q={search_responses[0]}&language=auto&time_range=&safesearch=0&categories=images",
        timeout=10,
        verify=False
    )
    

    valid_urls = []

    if include_images ==True:
        all_images_url = parse_images_search_results(response.text)
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(is_this_image_contextually_valid, user_prompt, url) for url in all_images_url[:15]]
            for future in concurrent.futures.as_completed(futures):
                url, result = future.result()
                if result.strip().lower().strip().rstrip(".") == "true":
                    valid_urls.append(url)


    final_output = {
        "content": summary_content.strip(),
        "reference_links": all_urls,
        "image_links": valid_urls
    }


    print(final_output)
    response_body = {"TEXT": {"body":json.dumps(final_output)}}

    action_response = {
        'actionGroup': action_group,
        'function': function,
        'functionResponse': {'responseBody': response_body}
    }

    agent_function_response = {
        'response': action_response,
        'messageVersion': event['messageVersion']
    }
    return agent_function_response


    
